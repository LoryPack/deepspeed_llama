project_name: 'llama'
fixed_parameters:
  output_dir: 'cache'
  num_logs_per_epoch: 1
  bf16: true
  num_gpus: 2
  deepspeed: true
  data_dir: 'data' 
  cpus_per_gpu: 10
  deepspeed_config: 'run/deepspeed.config' 
  gradient_checkpointing: true
  randomise_data_order: true
  eval_accumulation_steps_config: 1
  is_phases_training: false 
  is_openai_experiment: false
  ram_limit_gb: 460
  ignore_loss_on_prompt_tokens: true
  gradient_accumulation_steps: 8
hyperparameters:
  lr:
    - 0.001
    - 0.0001
    - 0.00003
    - 0.00001
    - 0.000003
    - 0.000001
    - 0.0000003
    - 0.0000001
  model_name:
    - 'llama-7b'
  train_path:
      - 'v1/finetuning_dataset_train_7b_prepared.jsonl'
  validation_path:
      - 'v1/finetuning_dataset_validation_7b_prepared.jsonl'
  num_epochs:
    - 10
  batch_size:
    - 32
